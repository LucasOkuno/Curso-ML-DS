{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fundamentos teóricos:\n",
    "- Prceptron\n",
    "- Treinamento/ajuste dos pesos\n",
    "- Gradient descent (descida do gradiente)\n",
    "- Cálculo do Delta\n",
    "- Learning rate (taxa de aprendizagem)\n",
    "- Momentum (momento)\n",
    "- Backpropagation\n",
    "\n",
    "OBS: Redes neurais são muito eficientes quando temos um grande volume de dados e para problemas complexos\n",
    "\n",
    "### Redes neurais \n",
    "- Não possi algoritmos pré-estabelecidos para solucionar os problemas\n",
    "- Imitar o sistema nervoso de humanos no processo de aprendizagem\n",
    "- Inspirada em redes neurais biológicas\n",
    "- Parecido com  a troca de informações em uma rede biológica \n",
    "- Com deep Learning as redes neurais ficaram populares novamente\n",
    "- pesos (são o conhecimento da rede neural) amplificam ou reduzem o sinal de entrada.\n",
    "\n",
    "OBS: step function (função degrau) é uma função em que pequenas modificações podem modificar o resultado final, que não é tão boa para uma rede neural. Necessita de uma função mais complexa em que pequenas modificações não tenham grande impacto no resultado final.\n",
    "\n",
    "OBS: Redes neurais multicamadas possui camada oculta (hidden layer) \n",
    "\n",
    "OBS: Operador XOR não se adequa a uma rede neural de uma só camada pois é um problema não linearmente separável\n",
    "\n",
    "### Funções de ativação (mais comuns)\n",
    "- Step Function (retorna valor 0 ou 1)\n",
    "- Sigmoid (valores entre 0 e 1)\n",
    "- Hyperbolic tangent (valores entre -1 e 1)\n",
    "\n",
    "\n",
    "### Algoritmo:\n",
    "- Inicializa os pesos com valores aleatórios\n",
    "- Baseado nos dados (aprendizagem supervisionada), realiza os cáluclos com os pesos e calcula o erro\n",
    "- Calcula as mudanças nos pesos e os atualiza (backpropagation)\n",
    "- O algoritmo termina quando o erro é pequeno\n",
    "\n",
    "OBS: Função que calcula o erro é chamada de cost function\n",
    "\n",
    "Gradiente -> encontrar a combinação de pesos que o erro é o menor possível e é calculado para saber quanto ajustar os pesos\n",
    "\n",
    "### Taxa de aprendizagem (Learning rate)\n",
    "- Define o quão rápido o algoritmo vai aprender\n",
    "- Alto: A convergência é rápida mas pode perder o mínimo global\n",
    "- Baixo: Será mais lento mas tem mais chances de chegar no mínimo global\n",
    "\n",
    "### Momento (Momentum)\n",
    "- Escapar de mínimos locais (nem sempre funciona)\n",
    "- Define o quão confiável é a última alteração\n",
    "- Alto: Aumenta a velocidade da convergencia\n",
    "- Baixo: Pode evitar mínimos locais\n",
    "\n",
    "### Mean square Error (MSE) e Root Mean square Error (RMSE)\n",
    "- Média da diferença entre o esperado e o que foi previsto pela rede\n",
    "- Erros maiores contam mais que erros  menores\n",
    "- Penaliza erros maiores\n",
    "\n",
    "OBS: Quando temos problemas mais complexos é necessário colocar mais neurônios na camada de saída.A abordagem mais comum é utilizar para a camada de saída a mesma quantidade de classes que temos.Por exemplo, na classificação de risco de crédito onde temos [ALTO, MÈDIO, BAIXO] (3 classes) utilizamos geralmente 3 neurônios na camada de saída.\n",
    "\n",
    "### Deep Learning\n",
    "- \"Problema do gradiente desaparecendo\" - vanishinggradiente problem - gradiente fica muito pequeno, mudança nos pesos ficam pequena\n",
    "- Utilização de outras técnicas e funções de ativação\n",
    "- Por onde seguir\n",
    "    - Redes neurais convolucionais\n",
    "    - Redes neurais recorrentes\n",
    "    - Keras, TensorFlow, Theano\n",
    "    - GPU\n",
    "### Camadas ocultas\n",
    "- Não existe um número exato de neurônios que deve ser usado mas existe uma fórmula bem classica que pode ser utilizada $Neuronios = \\frac{Entradas + Saídas}{2} $\n",
    "- Para descobrir qual a melhor parametrização pode ser utilizada a técnica de validação cruzada (cross validation)\n",
    "- Em geral, duas camadas funcionam bem para poucos dados\n",
    "\n",
    "OBS: Problemas linearmente separáveis não necessitam de camadas ocultas (operadores AND e OR)\n",
    "OBS: Quanto maior o valor da ativação, mais impacto o neurônio possui\n",
    "\n",
    "Podemos utilizar diferentes funções de ativação para a Camada oculta e para a camada de saída. Por exemplo utilizar a função de ativação Relu (Rectifier, muito utilizada em deep learning) para a camada oculta e a função sigmoide para a camada de saída. Para 1 neurônio de saida podemos utilizar a sigmoide e para mais de 1 neurônio podemos utilizar a função softmax (muito utlizada em Deep Learning, também)\n",
    "\n",
    "OBS: Para camada de saída categórica podemos fazer um Encoding, quando temos de 3 ou mais neurônios na camada de saída\n",
    "\n",
    "- Batch Gradiente Descent - Calcula o erro para todos os registros e atualiza os pesos\n",
    "    - Mini Batch Gradient Descent - Escolhe um número de registros para rodar e atualizas os pesos\n",
    "- Stochastic Gradient Descent - Calcula o erro para cada registro e atualiza os pesos\n",
    "    - Ajuda a prevenir minímos locais (superfícies não convexas)\n",
    "    - Mais rápido (não precisa carregar todos os dados em memória)\n",
    "    \n",
    "# Base de dados crédito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redes neurais artificiais (MLP = multilayer perceptron)\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('credit.pkl', 'rb') as f:\n",
    "    x_df_train, y_df_train, x_df_test, y_df_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network = MLPClassifier()\n",
    "neural_network.fit(x_df_train, y_df_train)\n",
    "#Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
    "# AQui vemos que a otimização ainda n convergiu entao aumentaremos o numero maximo de iteracoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70639853\n",
      "Iteration 2, loss = 0.64166365\n",
      "Iteration 3, loss = 0.58576731\n",
      "Iteration 4, loss = 0.53747080\n",
      "Iteration 5, loss = 0.49522885\n",
      "Iteration 6, loss = 0.45958523\n",
      "Iteration 7, loss = 0.42766289\n",
      "Iteration 8, loss = 0.40051574\n",
      "Iteration 9, loss = 0.37621202\n",
      "Iteration 10, loss = 0.35464003\n",
      "Iteration 11, loss = 0.33531262\n",
      "Iteration 12, loss = 0.31762453\n",
      "Iteration 13, loss = 0.30133985\n",
      "Iteration 14, loss = 0.28626154\n",
      "Iteration 15, loss = 0.27229772\n",
      "Iteration 16, loss = 0.25923142\n",
      "Iteration 17, loss = 0.24734944\n",
      "Iteration 18, loss = 0.23621782\n",
      "Iteration 19, loss = 0.22589197\n",
      "Iteration 20, loss = 0.21623713\n",
      "Iteration 21, loss = 0.20724447\n",
      "Iteration 22, loss = 0.19873579\n",
      "Iteration 23, loss = 0.19078728\n",
      "Iteration 24, loss = 0.18347338\n",
      "Iteration 25, loss = 0.17651975\n",
      "Iteration 26, loss = 0.17001859\n",
      "Iteration 27, loss = 0.16416478\n",
      "Iteration 28, loss = 0.15856581\n",
      "Iteration 29, loss = 0.15330485\n",
      "Iteration 30, loss = 0.14838612\n",
      "Iteration 31, loss = 0.14388866\n",
      "Iteration 32, loss = 0.13966757\n",
      "Iteration 33, loss = 0.13563815\n",
      "Iteration 34, loss = 0.13200615\n",
      "Iteration 35, loss = 0.12847028\n",
      "Iteration 36, loss = 0.12522335\n",
      "Iteration 37, loss = 0.12221916\n",
      "Iteration 38, loss = 0.11935808\n",
      "Iteration 39, loss = 0.11664669\n",
      "Iteration 40, loss = 0.11411134\n",
      "Iteration 41, loss = 0.11172103\n",
      "Iteration 42, loss = 0.10946417\n",
      "Iteration 43, loss = 0.10731767\n",
      "Iteration 44, loss = 0.10527305\n",
      "Iteration 45, loss = 0.10340485\n",
      "Iteration 46, loss = 0.10159999\n",
      "Iteration 47, loss = 0.09978256\n",
      "Iteration 48, loss = 0.09822256\n",
      "Iteration 49, loss = 0.09653409\n",
      "Iteration 50, loss = 0.09507798\n",
      "Iteration 51, loss = 0.09352926\n",
      "Iteration 52, loss = 0.09208923\n",
      "Iteration 53, loss = 0.09088831\n",
      "Iteration 54, loss = 0.08950273\n",
      "Iteration 55, loss = 0.08826069\n",
      "Iteration 56, loss = 0.08706471\n",
      "Iteration 57, loss = 0.08589454\n",
      "Iteration 58, loss = 0.08480106\n",
      "Iteration 59, loss = 0.08377979\n",
      "Iteration 60, loss = 0.08268957\n",
      "Iteration 61, loss = 0.08159411\n",
      "Iteration 62, loss = 0.08059689\n",
      "Iteration 63, loss = 0.07963933\n",
      "Iteration 64, loss = 0.07865755\n",
      "Iteration 65, loss = 0.07775981\n",
      "Iteration 66, loss = 0.07686857\n",
      "Iteration 67, loss = 0.07591251\n",
      "Iteration 68, loss = 0.07513119\n",
      "Iteration 69, loss = 0.07427301\n",
      "Iteration 70, loss = 0.07340258\n",
      "Iteration 71, loss = 0.07256399\n",
      "Iteration 72, loss = 0.07180250\n",
      "Iteration 73, loss = 0.07106262\n",
      "Iteration 74, loss = 0.07032166\n",
      "Iteration 75, loss = 0.06950092\n",
      "Iteration 76, loss = 0.06878438\n",
      "Iteration 77, loss = 0.06806761\n",
      "Iteration 78, loss = 0.06746351\n",
      "Iteration 79, loss = 0.06667716\n",
      "Iteration 80, loss = 0.06596967\n",
      "Iteration 81, loss = 0.06534262\n",
      "Iteration 82, loss = 0.06467942\n",
      "Iteration 83, loss = 0.06404619\n",
      "Iteration 84, loss = 0.06336988\n",
      "Iteration 85, loss = 0.06274071\n",
      "Iteration 86, loss = 0.06217028\n",
      "Iteration 87, loss = 0.06163076\n",
      "Iteration 88, loss = 0.06101639\n",
      "Iteration 89, loss = 0.06059488\n",
      "Iteration 90, loss = 0.05988753\n",
      "Iteration 91, loss = 0.05935767\n",
      "Iteration 92, loss = 0.05881199\n",
      "Iteration 93, loss = 0.05830443\n",
      "Iteration 94, loss = 0.05773862\n",
      "Iteration 95, loss = 0.05730443\n",
      "Iteration 96, loss = 0.05672695\n",
      "Iteration 97, loss = 0.05629960\n",
      "Iteration 98, loss = 0.05580926\n",
      "Iteration 99, loss = 0.05532223\n",
      "Iteration 100, loss = 0.05485362\n",
      "Iteration 101, loss = 0.05447090\n",
      "Iteration 102, loss = 0.05392528\n",
      "Iteration 103, loss = 0.05356126\n",
      "Iteration 104, loss = 0.05307135\n",
      "Iteration 105, loss = 0.05265420\n",
      "Iteration 106, loss = 0.05221845\n",
      "Iteration 107, loss = 0.05186649\n",
      "Iteration 108, loss = 0.05149368\n",
      "Iteration 109, loss = 0.05101833\n",
      "Iteration 110, loss = 0.05057726\n",
      "Iteration 111, loss = 0.05019670\n",
      "Iteration 112, loss = 0.04987292\n",
      "Iteration 113, loss = 0.04952109\n",
      "Iteration 114, loss = 0.04910715\n",
      "Iteration 115, loss = 0.04882224\n",
      "Iteration 116, loss = 0.04836633\n",
      "Iteration 117, loss = 0.04813185\n",
      "Iteration 118, loss = 0.04766788\n",
      "Iteration 119, loss = 0.04726570\n",
      "Iteration 120, loss = 0.04694852\n",
      "Iteration 121, loss = 0.04664533\n",
      "Iteration 122, loss = 0.04627300\n",
      "Iteration 123, loss = 0.04600103\n",
      "Iteration 124, loss = 0.04561416\n",
      "Iteration 125, loss = 0.04525443\n",
      "Iteration 126, loss = 0.04501484\n",
      "Iteration 127, loss = 0.04464519\n",
      "Iteration 128, loss = 0.04434981\n",
      "Iteration 129, loss = 0.04408927\n",
      "Iteration 130, loss = 0.04372419\n",
      "Iteration 131, loss = 0.04340844\n",
      "Iteration 132, loss = 0.04315266\n",
      "Iteration 133, loss = 0.04286994\n",
      "Iteration 134, loss = 0.04257189\n",
      "Iteration 135, loss = 0.04227533\n",
      "Iteration 136, loss = 0.04199335\n",
      "Iteration 137, loss = 0.04171097\n",
      "Iteration 138, loss = 0.04142245\n",
      "Iteration 139, loss = 0.04112611\n",
      "Iteration 140, loss = 0.04085734\n",
      "Iteration 141, loss = 0.04061822\n",
      "Iteration 142, loss = 0.04033973\n",
      "Iteration 143, loss = 0.04003895\n",
      "Iteration 144, loss = 0.03978002\n",
      "Iteration 145, loss = 0.03953562\n",
      "Iteration 146, loss = 0.03932183\n",
      "Iteration 147, loss = 0.03913834\n",
      "Iteration 148, loss = 0.03883038\n",
      "Iteration 149, loss = 0.03862968\n",
      "Iteration 150, loss = 0.03830242\n",
      "Iteration 151, loss = 0.03806467\n",
      "Iteration 152, loss = 0.03789672\n",
      "Iteration 153, loss = 0.03758623\n",
      "Iteration 154, loss = 0.03736571\n",
      "Iteration 155, loss = 0.03710754\n",
      "Iteration 156, loss = 0.03703282\n",
      "Iteration 157, loss = 0.03668832\n",
      "Iteration 158, loss = 0.03652592\n",
      "Iteration 159, loss = 0.03623638\n",
      "Iteration 160, loss = 0.03601689\n",
      "Iteration 161, loss = 0.03580603\n",
      "Iteration 162, loss = 0.03569638\n",
      "Iteration 163, loss = 0.03541644\n",
      "Iteration 164, loss = 0.03522199\n",
      "Iteration 165, loss = 0.03500288\n",
      "Iteration 166, loss = 0.03474942\n",
      "Iteration 167, loss = 0.03456922\n",
      "Iteration 168, loss = 0.03434711\n",
      "Iteration 169, loss = 0.03419991\n",
      "Iteration 170, loss = 0.03403949\n",
      "Iteration 171, loss = 0.03388566\n",
      "Iteration 172, loss = 0.03373298\n",
      "Iteration 173, loss = 0.03343310\n",
      "Iteration 174, loss = 0.03326451\n",
      "Iteration 175, loss = 0.03315035\n",
      "Iteration 176, loss = 0.03299540\n",
      "Iteration 177, loss = 0.03266204\n",
      "Iteration 178, loss = 0.03255210\n",
      "Iteration 179, loss = 0.03235668\n",
      "Iteration 180, loss = 0.03213857\n",
      "Iteration 181, loss = 0.03199507\n",
      "Iteration 182, loss = 0.03179516\n",
      "Iteration 183, loss = 0.03163633\n",
      "Iteration 184, loss = 0.03147923\n",
      "Iteration 185, loss = 0.03136485\n",
      "Iteration 186, loss = 0.03118403\n",
      "Iteration 187, loss = 0.03098800\n",
      "Iteration 188, loss = 0.03084325\n",
      "Iteration 189, loss = 0.03084392\n",
      "Iteration 190, loss = 0.03063279\n",
      "Iteration 191, loss = 0.03038682\n",
      "Iteration 192, loss = 0.03024057\n",
      "Iteration 193, loss = 0.03009286\n",
      "Iteration 194, loss = 0.02993147\n",
      "Iteration 195, loss = 0.02977857\n",
      "Iteration 196, loss = 0.02968877\n",
      "Iteration 197, loss = 0.02954218\n",
      "Iteration 198, loss = 0.02940000\n",
      "Iteration 199, loss = 0.02921029\n",
      "Iteration 200, loss = 0.02924555\n",
      "Iteration 201, loss = 0.02904138\n",
      "Iteration 202, loss = 0.02883738\n",
      "Iteration 203, loss = 0.02869675\n",
      "Iteration 204, loss = 0.02854160\n",
      "Iteration 205, loss = 0.02846218\n",
      "Iteration 206, loss = 0.02834656\n",
      "Iteration 207, loss = 0.02825221\n",
      "Iteration 208, loss = 0.02803607\n",
      "Iteration 209, loss = 0.02804159\n",
      "Iteration 210, loss = 0.02773423\n",
      "Iteration 211, loss = 0.02763312\n",
      "Iteration 212, loss = 0.02748652\n",
      "Iteration 213, loss = 0.02738775\n",
      "Iteration 214, loss = 0.02724335\n",
      "Iteration 215, loss = 0.02718399\n",
      "Iteration 216, loss = 0.02702867\n",
      "Iteration 217, loss = 0.02697474\n",
      "Iteration 218, loss = 0.02682731\n",
      "Iteration 219, loss = 0.02670312\n",
      "Iteration 220, loss = 0.02655126\n",
      "Iteration 221, loss = 0.02646117\n",
      "Iteration 222, loss = 0.02628621\n",
      "Iteration 223, loss = 0.02616646\n",
      "Iteration 224, loss = 0.02608166\n",
      "Iteration 225, loss = 0.02602107\n",
      "Iteration 226, loss = 0.02589777\n",
      "Iteration 227, loss = 0.02578750\n",
      "Iteration 228, loss = 0.02564068\n",
      "Iteration 229, loss = 0.02556902\n",
      "Iteration 230, loss = 0.02544207\n",
      "Iteration 231, loss = 0.02530375\n",
      "Iteration 232, loss = 0.02532243\n",
      "Iteration 233, loss = 0.02514557\n",
      "Iteration 234, loss = 0.02507279\n",
      "Iteration 235, loss = 0.02490130\n",
      "Iteration 236, loss = 0.02486899\n",
      "Iteration 237, loss = 0.02471472\n",
      "Iteration 238, loss = 0.02460633\n",
      "Iteration 239, loss = 0.02447405\n",
      "Iteration 240, loss = 0.02435664\n",
      "Iteration 241, loss = 0.02429768\n",
      "Iteration 242, loss = 0.02422262\n",
      "Iteration 243, loss = 0.02407286\n",
      "Iteration 244, loss = 0.02400442\n",
      "Iteration 245, loss = 0.02392529\n",
      "Iteration 246, loss = 0.02384605\n",
      "Iteration 247, loss = 0.02370952\n",
      "Iteration 248, loss = 0.02363676\n",
      "Iteration 249, loss = 0.02357746\n",
      "Iteration 250, loss = 0.02344688\n",
      "Iteration 251, loss = 0.02343480\n",
      "Iteration 252, loss = 0.02328401\n",
      "Iteration 253, loss = 0.02319004\n",
      "Iteration 254, loss = 0.02307714\n",
      "Iteration 255, loss = 0.02299988\n",
      "Iteration 256, loss = 0.02290104\n",
      "Iteration 257, loss = 0.02283149\n",
      "Iteration 258, loss = 0.02274151\n",
      "Iteration 259, loss = 0.02267996\n",
      "Iteration 260, loss = 0.02254856\n",
      "Iteration 261, loss = 0.02251461\n",
      "Iteration 262, loss = 0.02238012\n",
      "Iteration 263, loss = 0.02233060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 264, loss = 0.02222461\n",
      "Iteration 265, loss = 0.02218597\n",
      "Iteration 266, loss = 0.02208945\n",
      "Iteration 267, loss = 0.02197227\n",
      "Iteration 268, loss = 0.02188528\n",
      "Iteration 269, loss = 0.02187366\n",
      "Iteration 270, loss = 0.02175655\n",
      "Iteration 271, loss = 0.02169588\n",
      "Iteration 272, loss = 0.02163517\n",
      "Iteration 273, loss = 0.02156247\n",
      "Iteration 274, loss = 0.02145483\n",
      "Iteration 275, loss = 0.02133060\n",
      "Iteration 276, loss = 0.02128811\n",
      "Iteration 277, loss = 0.02122498\n",
      "Iteration 278, loss = 0.02120736\n",
      "Iteration 279, loss = 0.02109608\n",
      "Iteration 280, loss = 0.02096636\n",
      "Iteration 281, loss = 0.02095340\n",
      "Iteration 282, loss = 0.02086195\n",
      "Iteration 283, loss = 0.02077974\n",
      "Iteration 284, loss = 0.02070194\n",
      "Iteration 285, loss = 0.02064996\n",
      "Iteration 286, loss = 0.02057619\n",
      "Iteration 287, loss = 0.02053319\n",
      "Iteration 288, loss = 0.02042266\n",
      "Iteration 289, loss = 0.02033048\n",
      "Iteration 290, loss = 0.02024858\n",
      "Iteration 291, loss = 0.02018438\n",
      "Iteration 292, loss = 0.02014647\n",
      "Iteration 293, loss = 0.02005855\n",
      "Iteration 294, loss = 0.02013671\n",
      "Iteration 295, loss = 0.01989289\n",
      "Iteration 296, loss = 0.02001786\n",
      "Iteration 297, loss = 0.01979065\n",
      "Iteration 298, loss = 0.01974053\n",
      "Iteration 299, loss = 0.01965092\n",
      "Iteration 300, loss = 0.01962020\n",
      "Iteration 301, loss = 0.01953800\n",
      "Iteration 302, loss = 0.01951071\n",
      "Iteration 303, loss = 0.01944694\n",
      "Iteration 304, loss = 0.01930766\n",
      "Iteration 305, loss = 0.01925351\n",
      "Iteration 306, loss = 0.01928310\n",
      "Iteration 307, loss = 0.01910369\n",
      "Iteration 308, loss = 0.01905394\n",
      "Iteration 309, loss = 0.01901429\n",
      "Iteration 310, loss = 0.01893778\n",
      "Iteration 311, loss = 0.01891374\n",
      "Iteration 312, loss = 0.01887547\n",
      "Iteration 313, loss = 0.01879799\n",
      "Iteration 314, loss = 0.01866560\n",
      "Iteration 315, loss = 0.01866069\n",
      "Iteration 316, loss = 0.01864009\n",
      "Iteration 317, loss = 0.01856740\n",
      "Iteration 318, loss = 0.01840792\n",
      "Iteration 319, loss = 0.01840659\n",
      "Iteration 320, loss = 0.01833687\n",
      "Iteration 321, loss = 0.01835941\n",
      "Iteration 322, loss = 0.01822701\n",
      "Iteration 323, loss = 0.01816149\n",
      "Iteration 324, loss = 0.01814231\n",
      "Iteration 325, loss = 0.01804155\n",
      "Iteration 326, loss = 0.01798977\n",
      "Iteration 327, loss = 0.01802178\n",
      "Iteration 328, loss = 0.01786809\n",
      "Iteration 329, loss = 0.01783026\n",
      "Iteration 330, loss = 0.01784415\n",
      "Iteration 331, loss = 0.01780400\n",
      "Iteration 332, loss = 0.01766362\n",
      "Iteration 333, loss = 0.01767457\n",
      "Iteration 334, loss = 0.01760853\n",
      "Iteration 335, loss = 0.01749011\n",
      "Iteration 336, loss = 0.01752603\n",
      "Iteration 337, loss = 0.01748946\n",
      "Iteration 338, loss = 0.01742854\n",
      "Iteration 339, loss = 0.01731171\n",
      "Iteration 340, loss = 0.01725533\n",
      "Iteration 341, loss = 0.01723881\n",
      "Iteration 342, loss = 0.01727766\n",
      "Iteration 343, loss = 0.01715985\n",
      "Iteration 344, loss = 0.01701982\n",
      "Iteration 345, loss = 0.01700790\n",
      "Iteration 346, loss = 0.01692822\n",
      "Iteration 347, loss = 0.01688700\n",
      "Iteration 348, loss = 0.01694201\n",
      "Iteration 349, loss = 0.01686162\n",
      "Iteration 350, loss = 0.01673968\n",
      "Iteration 351, loss = 0.01670667\n",
      "Iteration 352, loss = 0.01668365\n",
      "Iteration 353, loss = 0.01656922\n",
      "Iteration 354, loss = 0.01656682\n",
      "Iteration 355, loss = 0.01648486\n",
      "Iteration 356, loss = 0.01661700\n",
      "Iteration 357, loss = 0.01644107\n",
      "Iteration 358, loss = 0.01635136\n",
      "Iteration 359, loss = 0.01628601\n",
      "Iteration 360, loss = 0.01628660\n",
      "Iteration 361, loss = 0.01632533\n",
      "Iteration 362, loss = 0.01623430\n",
      "Iteration 363, loss = 0.01626494\n",
      "Iteration 364, loss = 0.01609266\n",
      "Iteration 365, loss = 0.01601844\n",
      "Iteration 366, loss = 0.01601995\n",
      "Iteration 367, loss = 0.01596572\n",
      "Iteration 368, loss = 0.01589481\n",
      "Iteration 369, loss = 0.01584696\n",
      "Iteration 370, loss = 0.01585604\n",
      "Iteration 371, loss = 0.01578833\n",
      "Iteration 372, loss = 0.01569855\n",
      "Iteration 373, loss = 0.01567143\n",
      "Iteration 374, loss = 0.01563949\n",
      "Iteration 375, loss = 0.01559765\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(max_iter=1000, verbose=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network = MLPClassifier(max_iter=1000, verbose=True)#verbose mostra as msg epoca por epoca\n",
    "neural_network.fit(x_df_train, y_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculo do numero de neuronios na camada oculta (3 entradas + 1 neuronio na camada de saida)\n",
    "(3+1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.05081518\n",
      "Iteration 2, loss = 1.04350196\n",
      "Iteration 3, loss = 1.03594632\n",
      "Iteration 4, loss = 1.02775905\n",
      "Iteration 5, loss = 1.01935608\n",
      "Iteration 6, loss = 1.01069509\n",
      "Iteration 7, loss = 1.00193423\n",
      "Iteration 8, loss = 0.99274919\n",
      "Iteration 9, loss = 0.98353771\n",
      "Iteration 10, loss = 0.97396818\n",
      "Iteration 11, loss = 0.96435712\n",
      "Iteration 12, loss = 0.95428819\n",
      "Iteration 13, loss = 0.94396410\n",
      "Iteration 14, loss = 0.93344448\n",
      "Iteration 15, loss = 0.92251573\n",
      "Iteration 16, loss = 0.91128171\n",
      "Iteration 17, loss = 0.89964183\n",
      "Iteration 18, loss = 0.88782467\n",
      "Iteration 19, loss = 0.87555647\n",
      "Iteration 20, loss = 0.86318304\n",
      "Iteration 21, loss = 0.85052888\n",
      "Iteration 22, loss = 0.83750624\n",
      "Iteration 23, loss = 0.82412513\n",
      "Iteration 24, loss = 0.81082194\n",
      "Iteration 25, loss = 0.79725647\n",
      "Iteration 26, loss = 0.78346298\n",
      "Iteration 27, loss = 0.76952634\n",
      "Iteration 28, loss = 0.75551726\n",
      "Iteration 29, loss = 0.74151759\n",
      "Iteration 30, loss = 0.72731567\n",
      "Iteration 31, loss = 0.71316314\n",
      "Iteration 32, loss = 0.69904120\n",
      "Iteration 33, loss = 0.68508161\n",
      "Iteration 34, loss = 0.67104601\n",
      "Iteration 35, loss = 0.65736912\n",
      "Iteration 36, loss = 0.64370469\n",
      "Iteration 37, loss = 0.63028185\n",
      "Iteration 38, loss = 0.61714786\n",
      "Iteration 39, loss = 0.60404669\n",
      "Iteration 40, loss = 0.59118518\n",
      "Iteration 41, loss = 0.57868276\n",
      "Iteration 42, loss = 0.56644589\n",
      "Iteration 43, loss = 0.55450702\n",
      "Iteration 44, loss = 0.54275570\n",
      "Iteration 45, loss = 0.53135522\n",
      "Iteration 46, loss = 0.52063960\n",
      "Iteration 47, loss = 0.51003189\n",
      "Iteration 48, loss = 0.49995475\n",
      "Iteration 49, loss = 0.48998294\n",
      "Iteration 50, loss = 0.48062243\n",
      "Iteration 51, loss = 0.47159304\n",
      "Iteration 52, loss = 0.46287033\n",
      "Iteration 53, loss = 0.45460176\n",
      "Iteration 54, loss = 0.44655966\n",
      "Iteration 55, loss = 0.43888759\n",
      "Iteration 56, loss = 0.43166380\n",
      "Iteration 57, loss = 0.42480456\n",
      "Iteration 58, loss = 0.41800906\n",
      "Iteration 59, loss = 0.41163164\n",
      "Iteration 60, loss = 0.40555470\n",
      "Iteration 61, loss = 0.39970950\n",
      "Iteration 62, loss = 0.39424300\n",
      "Iteration 63, loss = 0.38892683\n",
      "Iteration 64, loss = 0.38392272\n",
      "Iteration 65, loss = 0.37907479\n",
      "Iteration 66, loss = 0.37440681\n",
      "Iteration 67, loss = 0.37003193\n",
      "Iteration 68, loss = 0.36583042\n",
      "Iteration 69, loss = 0.36170328\n",
      "Iteration 70, loss = 0.35781487\n",
      "Iteration 71, loss = 0.35416235\n",
      "Iteration 72, loss = 0.35061275\n",
      "Iteration 73, loss = 0.34706051\n",
      "Iteration 74, loss = 0.34380046\n",
      "Iteration 75, loss = 0.34055442\n",
      "Iteration 76, loss = 0.33745837\n",
      "Iteration 77, loss = 0.33451855\n",
      "Iteration 78, loss = 0.33169262\n",
      "Iteration 79, loss = 0.32896853\n",
      "Iteration 80, loss = 0.32634620\n",
      "Iteration 81, loss = 0.32375331\n",
      "Iteration 82, loss = 0.32124369\n",
      "Iteration 83, loss = 0.31888732\n",
      "Iteration 84, loss = 0.31650713\n",
      "Iteration 85, loss = 0.31422687\n",
      "Iteration 86, loss = 0.31204039\n",
      "Iteration 87, loss = 0.30988218\n",
      "Iteration 88, loss = 0.30783802\n",
      "Iteration 89, loss = 0.30574871\n",
      "Iteration 90, loss = 0.30376173\n",
      "Iteration 91, loss = 0.30180653\n",
      "Iteration 92, loss = 0.29987913\n",
      "Iteration 93, loss = 0.29800437\n",
      "Iteration 94, loss = 0.29617761\n",
      "Iteration 95, loss = 0.29436851\n",
      "Iteration 96, loss = 0.29259988\n",
      "Iteration 97, loss = 0.29083496\n",
      "Iteration 98, loss = 0.28911268\n",
      "Iteration 99, loss = 0.28743859\n",
      "Iteration 100, loss = 0.28572138\n",
      "Iteration 101, loss = 0.28407851\n",
      "Iteration 102, loss = 0.28243861\n",
      "Iteration 103, loss = 0.28088600\n",
      "Iteration 104, loss = 0.27925964\n",
      "Iteration 105, loss = 0.27773540\n",
      "Iteration 106, loss = 0.27623989\n",
      "Iteration 107, loss = 0.27474019\n",
      "Iteration 108, loss = 0.27328968\n",
      "Iteration 109, loss = 0.27193903\n",
      "Iteration 110, loss = 0.27050345\n",
      "Iteration 111, loss = 0.26911987\n",
      "Iteration 112, loss = 0.26777961\n",
      "Iteration 113, loss = 0.26648017\n",
      "Iteration 114, loss = 0.26518527\n",
      "Iteration 115, loss = 0.26387695\n",
      "Iteration 116, loss = 0.26262219\n",
      "Iteration 117, loss = 0.26136181\n",
      "Iteration 118, loss = 0.26014252\n",
      "Iteration 119, loss = 0.25890552\n",
      "Iteration 120, loss = 0.25769758\n",
      "Iteration 121, loss = 0.25651103\n",
      "Iteration 122, loss = 0.25531969\n",
      "Iteration 123, loss = 0.25415108\n",
      "Iteration 124, loss = 0.25299770\n",
      "Iteration 125, loss = 0.25187341\n",
      "Iteration 126, loss = 0.25071867\n",
      "Iteration 127, loss = 0.24958696\n",
      "Iteration 128, loss = 0.24847340\n",
      "Iteration 129, loss = 0.24736443\n",
      "Iteration 130, loss = 0.24626449\n",
      "Iteration 131, loss = 0.24516871\n",
      "Iteration 132, loss = 0.24412112\n",
      "Iteration 133, loss = 0.24310212\n",
      "Iteration 134, loss = 0.24206602\n",
      "Iteration 135, loss = 0.24109373\n",
      "Iteration 136, loss = 0.24010576\n",
      "Iteration 137, loss = 0.23914596\n",
      "Iteration 138, loss = 0.23820960\n",
      "Iteration 139, loss = 0.23726130\n",
      "Iteration 140, loss = 0.23631764\n",
      "Iteration 141, loss = 0.23542743\n",
      "Iteration 142, loss = 0.23452362\n",
      "Iteration 143, loss = 0.23364612\n",
      "Iteration 144, loss = 0.23275931\n",
      "Iteration 145, loss = 0.23190442\n",
      "Iteration 146, loss = 0.23103105\n",
      "Iteration 147, loss = 0.23019290\n",
      "Iteration 148, loss = 0.22931173\n",
      "Iteration 149, loss = 0.22848769\n",
      "Iteration 150, loss = 0.22763734\n",
      "Iteration 151, loss = 0.22681009\n",
      "Iteration 152, loss = 0.22599558\n",
      "Iteration 153, loss = 0.22513882\n",
      "Iteration 154, loss = 0.22436334\n",
      "Iteration 155, loss = 0.22350901\n",
      "Iteration 156, loss = 0.22272122\n",
      "Iteration 157, loss = 0.22189527\n",
      "Iteration 158, loss = 0.22113059\n",
      "Iteration 159, loss = 0.22034015\n",
      "Iteration 160, loss = 0.21955232\n",
      "Iteration 161, loss = 0.21878561\n",
      "Iteration 162, loss = 0.21802462\n",
      "Iteration 163, loss = 0.21728002\n",
      "Iteration 164, loss = 0.21652625\n",
      "Iteration 165, loss = 0.21577462\n",
      "Iteration 166, loss = 0.21503025\n",
      "Iteration 167, loss = 0.21427834\n",
      "Iteration 168, loss = 0.21353902\n",
      "Iteration 169, loss = 0.21279585\n",
      "Iteration 170, loss = 0.21202160\n",
      "Iteration 171, loss = 0.21130973\n",
      "Iteration 172, loss = 0.21056025\n",
      "Iteration 173, loss = 0.20982124\n",
      "Iteration 174, loss = 0.20911310\n",
      "Iteration 175, loss = 0.20838724\n",
      "Iteration 176, loss = 0.20768185\n",
      "Iteration 177, loss = 0.20693496\n",
      "Iteration 178, loss = 0.20621495\n",
      "Iteration 179, loss = 0.20547253\n",
      "Iteration 180, loss = 0.20477192\n",
      "Iteration 181, loss = 0.20402483\n",
      "Iteration 182, loss = 0.20329721\n",
      "Iteration 183, loss = 0.20258498\n",
      "Iteration 184, loss = 0.20187259\n",
      "Iteration 185, loss = 0.20116854\n",
      "Iteration 186, loss = 0.20048900\n",
      "Iteration 187, loss = 0.19976100\n",
      "Iteration 188, loss = 0.19908780\n",
      "Iteration 189, loss = 0.19842759\n",
      "Iteration 190, loss = 0.19773845\n",
      "Iteration 191, loss = 0.19706257\n",
      "Iteration 192, loss = 0.19641401\n",
      "Iteration 193, loss = 0.19574223\n",
      "Iteration 194, loss = 0.19507868\n",
      "Iteration 195, loss = 0.19440649\n",
      "Iteration 196, loss = 0.19378047\n",
      "Iteration 197, loss = 0.19308109\n",
      "Iteration 198, loss = 0.19244705\n",
      "Iteration 199, loss = 0.19178957\n",
      "Iteration 200, loss = 0.19112835\n",
      "Iteration 201, loss = 0.19049604\n",
      "Iteration 202, loss = 0.18980463\n",
      "Iteration 203, loss = 0.18915986\n",
      "Iteration 204, loss = 0.18851975\n",
      "Iteration 205, loss = 0.18786919\n",
      "Iteration 206, loss = 0.18721118\n",
      "Iteration 207, loss = 0.18660312\n",
      "Iteration 208, loss = 0.18597680\n",
      "Iteration 209, loss = 0.18537058\n",
      "Iteration 210, loss = 0.18475046\n",
      "Iteration 211, loss = 0.18416985\n",
      "Iteration 212, loss = 0.18357034\n",
      "Iteration 213, loss = 0.18298794\n",
      "Iteration 214, loss = 0.18241672\n",
      "Iteration 215, loss = 0.18182954\n",
      "Iteration 216, loss = 0.18124598\n",
      "Iteration 217, loss = 0.18067206\n",
      "Iteration 218, loss = 0.18008128\n",
      "Iteration 219, loss = 0.17948748\n",
      "Iteration 220, loss = 0.17889307\n",
      "Iteration 221, loss = 0.17832899\n",
      "Iteration 222, loss = 0.17772337\n",
      "Iteration 223, loss = 0.17713439\n",
      "Iteration 224, loss = 0.17657229\n",
      "Iteration 225, loss = 0.17595556\n",
      "Iteration 226, loss = 0.17537476\n",
      "Iteration 227, loss = 0.17478200\n",
      "Iteration 228, loss = 0.17419445\n",
      "Iteration 229, loss = 0.17361091\n",
      "Iteration 230, loss = 0.17305862\n",
      "Iteration 231, loss = 0.17251430\n",
      "Iteration 232, loss = 0.17191836\n",
      "Iteration 233, loss = 0.17135993\n",
      "Iteration 234, loss = 0.17080212\n",
      "Iteration 235, loss = 0.17027181\n",
      "Iteration 236, loss = 0.16970517\n",
      "Iteration 237, loss = 0.16916359\n",
      "Iteration 238, loss = 0.16863340\n",
      "Iteration 239, loss = 0.16808796\n",
      "Iteration 240, loss = 0.16757474\n",
      "Iteration 241, loss = 0.16706087\n",
      "Iteration 242, loss = 0.16653808\n",
      "Iteration 243, loss = 0.16602040\n",
      "Iteration 244, loss = 0.16552741\n",
      "Iteration 245, loss = 0.16503222\n",
      "Iteration 246, loss = 0.16454730\n",
      "Iteration 247, loss = 0.16406336\n",
      "Iteration 248, loss = 0.16358481\n",
      "Iteration 249, loss = 0.16312147\n",
      "Iteration 250, loss = 0.16264523\n",
      "Iteration 251, loss = 0.16216580\n",
      "Iteration 252, loss = 0.16173295\n",
      "Iteration 253, loss = 0.16125308\n",
      "Iteration 254, loss = 0.16080099\n",
      "Iteration 255, loss = 0.16034924\n",
      "Iteration 256, loss = 0.15990293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 257, loss = 0.15946092\n",
      "Iteration 258, loss = 0.15901350\n",
      "Iteration 259, loss = 0.15855657\n",
      "Iteration 260, loss = 0.15811142\n",
      "Iteration 261, loss = 0.15766577\n",
      "Iteration 262, loss = 0.15722368\n",
      "Iteration 263, loss = 0.15678836\n",
      "Iteration 264, loss = 0.15635376\n",
      "Iteration 265, loss = 0.15590964\n",
      "Iteration 266, loss = 0.15548890\n",
      "Iteration 267, loss = 0.15506572\n",
      "Iteration 268, loss = 0.15464677\n",
      "Iteration 269, loss = 0.15421698\n",
      "Iteration 270, loss = 0.15380262\n",
      "Iteration 271, loss = 0.15338896\n",
      "Iteration 272, loss = 0.15298015\n",
      "Iteration 273, loss = 0.15256857\n",
      "Iteration 274, loss = 0.15215873\n",
      "Iteration 275, loss = 0.15175022\n",
      "Iteration 276, loss = 0.15135989\n",
      "Iteration 277, loss = 0.15095785\n",
      "Iteration 278, loss = 0.15055483\n",
      "Iteration 279, loss = 0.15017686\n",
      "Iteration 280, loss = 0.14978205\n",
      "Iteration 281, loss = 0.14938410\n",
      "Iteration 282, loss = 0.14899092\n",
      "Iteration 283, loss = 0.14860593\n",
      "Iteration 284, loss = 0.14820958\n",
      "Iteration 285, loss = 0.14781448\n",
      "Iteration 286, loss = 0.14742334\n",
      "Iteration 287, loss = 0.14703540\n",
      "Iteration 288, loss = 0.14663366\n",
      "Iteration 289, loss = 0.14626206\n",
      "Iteration 290, loss = 0.14588274\n",
      "Iteration 291, loss = 0.14550438\n",
      "Iteration 292, loss = 0.14515200\n",
      "Iteration 293, loss = 0.14476382\n",
      "Iteration 294, loss = 0.14441558\n",
      "Iteration 295, loss = 0.14402383\n",
      "Iteration 296, loss = 0.14364014\n",
      "Iteration 297, loss = 0.14329619\n",
      "Iteration 298, loss = 0.14294581\n",
      "Iteration 299, loss = 0.14269142\n",
      "Iteration 300, loss = 0.14243813\n",
      "Iteration 301, loss = 0.14220944\n",
      "Iteration 302, loss = 0.14200117\n",
      "Iteration 303, loss = 0.14178096\n",
      "Iteration 304, loss = 0.14156347\n",
      "Iteration 305, loss = 0.14135567\n",
      "Iteration 306, loss = 0.14114621\n",
      "Iteration 307, loss = 0.14094063\n",
      "Iteration 308, loss = 0.14074122\n",
      "Iteration 309, loss = 0.14054460\n",
      "Iteration 310, loss = 0.14033837\n",
      "Iteration 311, loss = 0.14013945\n",
      "Iteration 312, loss = 0.13994047\n",
      "Iteration 313, loss = 0.13972916\n",
      "Iteration 314, loss = 0.13954279\n",
      "Iteration 315, loss = 0.13935548\n",
      "Iteration 316, loss = 0.13916349\n",
      "Iteration 317, loss = 0.13897462\n",
      "Iteration 318, loss = 0.13878315\n",
      "Iteration 319, loss = 0.13861340\n",
      "Iteration 320, loss = 0.13840648\n",
      "Iteration 321, loss = 0.13822039\n",
      "Iteration 322, loss = 0.13804298\n",
      "Iteration 323, loss = 0.13784309\n",
      "Iteration 324, loss = 0.13766633\n",
      "Iteration 325, loss = 0.13746976\n",
      "Iteration 326, loss = 0.13729145\n",
      "Iteration 327, loss = 0.13711604\n",
      "Iteration 328, loss = 0.13694224\n",
      "Iteration 329, loss = 0.13675315\n",
      "Iteration 330, loss = 0.13657211\n",
      "Iteration 331, loss = 0.13638804\n",
      "Iteration 332, loss = 0.13620318\n",
      "Iteration 333, loss = 0.13603524\n",
      "Iteration 334, loss = 0.13587559\n",
      "Iteration 335, loss = 0.13569820\n",
      "Iteration 336, loss = 0.13552426\n",
      "Iteration 337, loss = 0.13534802\n",
      "Iteration 338, loss = 0.13518393\n",
      "Iteration 339, loss = 0.13500080\n",
      "Iteration 340, loss = 0.13484918\n",
      "Iteration 341, loss = 0.13466160\n",
      "Iteration 342, loss = 0.13450215\n",
      "Iteration 343, loss = 0.13431801\n",
      "Iteration 344, loss = 0.13413948\n",
      "Iteration 345, loss = 0.13398135\n",
      "Iteration 346, loss = 0.13381674\n",
      "Iteration 347, loss = 0.13362969\n",
      "Iteration 348, loss = 0.13346995\n",
      "Iteration 349, loss = 0.13330543\n",
      "Iteration 350, loss = 0.13311005\n",
      "Iteration 351, loss = 0.13296983\n",
      "Iteration 352, loss = 0.13279022\n",
      "Iteration 353, loss = 0.13261656\n",
      "Iteration 354, loss = 0.13244367\n",
      "Iteration 355, loss = 0.13228007\n",
      "Iteration 356, loss = 0.13209561\n",
      "Iteration 357, loss = 0.13190688\n",
      "Iteration 358, loss = 0.13174779\n",
      "Iteration 359, loss = 0.13157444\n",
      "Iteration 360, loss = 0.13143565\n",
      "Iteration 361, loss = 0.13122870\n",
      "Iteration 362, loss = 0.13107109\n",
      "Iteration 363, loss = 0.13092109\n",
      "Iteration 364, loss = 0.13075800\n",
      "Iteration 365, loss = 0.13060283\n",
      "Iteration 366, loss = 0.13043848\n",
      "Iteration 367, loss = 0.13027807\n",
      "Iteration 368, loss = 0.13014138\n",
      "Iteration 369, loss = 0.12997254\n",
      "Iteration 370, loss = 0.12981649\n",
      "Iteration 371, loss = 0.12962173\n",
      "Iteration 372, loss = 0.12952570\n",
      "Iteration 373, loss = 0.12932138\n",
      "Iteration 374, loss = 0.12915402\n",
      "Iteration 375, loss = 0.12902280\n",
      "Iteration 376, loss = 0.12886653\n",
      "Iteration 377, loss = 0.12869879\n",
      "Iteration 378, loss = 0.12858052\n",
      "Iteration 379, loss = 0.12839739\n",
      "Iteration 380, loss = 0.12826707\n",
      "Iteration 381, loss = 0.12809104\n",
      "Iteration 382, loss = 0.12791606\n",
      "Iteration 383, loss = 0.12777489\n",
      "Iteration 384, loss = 0.12763067\n",
      "Iteration 385, loss = 0.12748970\n",
      "Iteration 386, loss = 0.12732587\n",
      "Iteration 387, loss = 0.12718159\n",
      "Iteration 388, loss = 0.12703179\n",
      "Iteration 389, loss = 0.12689252\n",
      "Iteration 390, loss = 0.12674860\n",
      "Iteration 391, loss = 0.12660364\n",
      "Iteration 392, loss = 0.12644244\n",
      "Iteration 393, loss = 0.12631377\n",
      "Iteration 394, loss = 0.12619001\n",
      "Iteration 395, loss = 0.12607325\n",
      "Iteration 396, loss = 0.12590158\n",
      "Iteration 397, loss = 0.12578112\n",
      "Iteration 398, loss = 0.12565352\n",
      "Iteration 399, loss = 0.12554204\n",
      "Iteration 400, loss = 0.12538915\n",
      "Iteration 401, loss = 0.12525100\n",
      "Iteration 402, loss = 0.12511724\n",
      "Iteration 403, loss = 0.12497351\n",
      "Iteration 404, loss = 0.12485200\n",
      "Iteration 405, loss = 0.12469936\n",
      "Iteration 406, loss = 0.12456521\n",
      "Iteration 407, loss = 0.12444808\n",
      "Iteration 408, loss = 0.12430395\n",
      "Iteration 409, loss = 0.12416907\n",
      "Iteration 410, loss = 0.12403677\n",
      "Iteration 411, loss = 0.12389568\n",
      "Iteration 412, loss = 0.12377520\n",
      "Iteration 413, loss = 0.12364803\n",
      "Iteration 414, loss = 0.12350521\n",
      "Iteration 415, loss = 0.12341476\n",
      "Iteration 416, loss = 0.12325609\n",
      "Iteration 417, loss = 0.12312468\n",
      "Iteration 418, loss = 0.12301982\n",
      "Iteration 419, loss = 0.12290971\n",
      "Iteration 420, loss = 0.12273789\n",
      "Iteration 421, loss = 0.12261865\n",
      "Iteration 422, loss = 0.12249227\n",
      "Iteration 423, loss = 0.12241583\n",
      "Iteration 424, loss = 0.12226503\n",
      "Iteration 425, loss = 0.12212573\n",
      "Iteration 426, loss = 0.12202301\n",
      "Iteration 427, loss = 0.12189087\n",
      "Iteration 428, loss = 0.12179838\n",
      "Iteration 429, loss = 0.12166356\n",
      "Iteration 430, loss = 0.12158229\n",
      "Iteration 431, loss = 0.12143179\n",
      "Iteration 432, loss = 0.12128988\n",
      "Iteration 433, loss = 0.12118102\n",
      "Iteration 434, loss = 0.12105502\n",
      "Iteration 435, loss = 0.12096611\n",
      "Iteration 436, loss = 0.12082664\n",
      "Iteration 437, loss = 0.12071197\n",
      "Iteration 438, loss = 0.12062259\n",
      "Iteration 439, loss = 0.12048400\n",
      "Iteration 440, loss = 0.12038347\n",
      "Iteration 441, loss = 0.12027362\n",
      "Iteration 442, loss = 0.12014742\n",
      "Iteration 443, loss = 0.12003978\n",
      "Iteration 444, loss = 0.11992261\n",
      "Iteration 445, loss = 0.11982771\n",
      "Iteration 446, loss = 0.11972324\n",
      "Iteration 447, loss = 0.11962502\n",
      "Iteration 448, loss = 0.11952034\n",
      "Iteration 449, loss = 0.11939208\n",
      "Iteration 450, loss = 0.11928574\n",
      "Iteration 451, loss = 0.11919074\n",
      "Iteration 452, loss = 0.11908959\n",
      "Iteration 453, loss = 0.11896191\n",
      "Iteration 454, loss = 0.11888859\n",
      "Iteration 455, loss = 0.11875945\n",
      "Iteration 456, loss = 0.11866769\n",
      "Iteration 457, loss = 0.11853832\n",
      "Iteration 458, loss = 0.11843939\n",
      "Iteration 459, loss = 0.11834173\n",
      "Iteration 460, loss = 0.11821712\n",
      "Iteration 461, loss = 0.11812463\n",
      "Iteration 462, loss = 0.11801376\n",
      "Iteration 463, loss = 0.11791782\n",
      "Iteration 464, loss = 0.11783916\n",
      "Iteration 465, loss = 0.11771149\n",
      "Iteration 466, loss = 0.11762862\n",
      "Iteration 467, loss = 0.11749332\n",
      "Iteration 468, loss = 0.11737386\n",
      "Iteration 469, loss = 0.11727641\n",
      "Iteration 470, loss = 0.11714845\n",
      "Iteration 471, loss = 0.11705417\n",
      "Iteration 472, loss = 0.11692396\n",
      "Iteration 473, loss = 0.11686268\n",
      "Iteration 474, loss = 0.11671067\n",
      "Iteration 475, loss = 0.11658638\n",
      "Iteration 476, loss = 0.11650012\n",
      "Iteration 477, loss = 0.11636930\n",
      "Iteration 478, loss = 0.11626689\n",
      "Iteration 479, loss = 0.11613045\n",
      "Iteration 480, loss = 0.11600745\n",
      "Iteration 481, loss = 0.11587304\n",
      "Iteration 482, loss = 0.11574843\n",
      "Iteration 483, loss = 0.11562844\n",
      "Iteration 484, loss = 0.11550624\n",
      "Iteration 485, loss = 0.11536148\n",
      "Iteration 486, loss = 0.11527201\n",
      "Iteration 487, loss = 0.11513601\n",
      "Iteration 488, loss = 0.11502195\n",
      "Iteration 489, loss = 0.11490433\n",
      "Iteration 490, loss = 0.11479354\n",
      "Iteration 491, loss = 0.11470909\n",
      "Iteration 492, loss = 0.11457080\n",
      "Iteration 493, loss = 0.11445653\n",
      "Iteration 494, loss = 0.11435426\n",
      "Iteration 495, loss = 0.11418432\n",
      "Iteration 496, loss = 0.11410449\n",
      "Iteration 497, loss = 0.11392242\n",
      "Iteration 498, loss = 0.11387680\n",
      "Iteration 499, loss = 0.11367913\n",
      "Iteration 500, loss = 0.11355705\n",
      "Iteration 501, loss = 0.11340451\n",
      "Iteration 502, loss = 0.11327613\n",
      "Iteration 503, loss = 0.11315124\n",
      "Iteration 504, loss = 0.11299317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 505, loss = 0.11285538\n",
      "Iteration 506, loss = 0.11271572\n",
      "Iteration 507, loss = 0.11259192\n",
      "Iteration 508, loss = 0.11241839\n",
      "Iteration 509, loss = 0.11226501\n",
      "Iteration 510, loss = 0.11212950\n",
      "Iteration 511, loss = 0.11199849\n",
      "Iteration 512, loss = 0.11185030\n",
      "Iteration 513, loss = 0.11172160\n",
      "Iteration 514, loss = 0.11158046\n",
      "Iteration 515, loss = 0.11143903\n",
      "Iteration 516, loss = 0.11127215\n",
      "Iteration 517, loss = 0.11114772\n",
      "Iteration 518, loss = 0.11096106\n",
      "Iteration 519, loss = 0.11080103\n",
      "Iteration 520, loss = 0.11063402\n",
      "Iteration 521, loss = 0.11046545\n",
      "Iteration 522, loss = 0.11032591\n",
      "Iteration 523, loss = 0.11012319\n",
      "Iteration 524, loss = 0.10994141\n",
      "Iteration 525, loss = 0.10976084\n",
      "Iteration 526, loss = 0.10964297\n",
      "Iteration 527, loss = 0.10940164\n",
      "Iteration 528, loss = 0.10916999\n",
      "Iteration 529, loss = 0.10897297\n",
      "Iteration 530, loss = 0.10874312\n",
      "Iteration 531, loss = 0.10849995\n",
      "Iteration 532, loss = 0.10827438\n",
      "Iteration 533, loss = 0.10805192\n",
      "Iteration 534, loss = 0.10779079\n",
      "Iteration 535, loss = 0.10753833\n",
      "Iteration 536, loss = 0.10729440\n",
      "Iteration 537, loss = 0.10705366\n",
      "Iteration 538, loss = 0.10676864\n",
      "Iteration 539, loss = 0.10651731\n",
      "Iteration 540, loss = 0.10626589\n",
      "Iteration 541, loss = 0.10598192\n",
      "Iteration 542, loss = 0.10569907\n",
      "Iteration 543, loss = 0.10539021\n",
      "Iteration 544, loss = 0.10497580\n",
      "Iteration 545, loss = 0.10458488\n",
      "Iteration 546, loss = 0.10413254\n",
      "Iteration 547, loss = 0.10375691\n",
      "Iteration 548, loss = 0.10334725\n",
      "Iteration 549, loss = 0.10291882\n",
      "Iteration 550, loss = 0.10245252\n",
      "Iteration 551, loss = 0.10193477\n",
      "Iteration 552, loss = 0.10123586\n",
      "Iteration 553, loss = 0.10058640\n",
      "Iteration 554, loss = 0.09984476\n",
      "Iteration 555, loss = 0.09894617\n",
      "Iteration 556, loss = 0.09821918\n",
      "Iteration 557, loss = 0.09748549\n",
      "Iteration 558, loss = 0.09672731\n",
      "Iteration 559, loss = 0.09600215\n",
      "Iteration 560, loss = 0.09515579\n",
      "Iteration 561, loss = 0.09424667\n",
      "Iteration 562, loss = 0.09331782\n",
      "Iteration 563, loss = 0.09232452\n",
      "Iteration 564, loss = 0.09166036\n",
      "Iteration 565, loss = 0.09090233\n",
      "Iteration 566, loss = 0.09015392\n",
      "Iteration 567, loss = 0.08954162\n",
      "Iteration 568, loss = 0.08889413\n",
      "Iteration 569, loss = 0.08836712\n",
      "Iteration 570, loss = 0.08786707\n",
      "Iteration 571, loss = 0.08734417\n",
      "Iteration 572, loss = 0.08686965\n",
      "Iteration 573, loss = 0.08639805\n",
      "Iteration 574, loss = 0.08592838\n",
      "Iteration 575, loss = 0.08547063\n",
      "Iteration 576, loss = 0.08505805\n",
      "Iteration 577, loss = 0.08465027\n",
      "Iteration 578, loss = 0.08430014\n",
      "Iteration 579, loss = 0.08392595\n",
      "Iteration 580, loss = 0.08355691\n",
      "Iteration 581, loss = 0.08314329\n",
      "Iteration 582, loss = 0.08284938\n",
      "Iteration 583, loss = 0.08243179\n",
      "Iteration 584, loss = 0.08205573\n",
      "Iteration 585, loss = 0.08167051\n",
      "Iteration 586, loss = 0.08132122\n",
      "Iteration 587, loss = 0.08096243\n",
      "Iteration 588, loss = 0.08060744\n",
      "Iteration 589, loss = 0.08025044\n",
      "Iteration 590, loss = 0.07989671\n",
      "Iteration 591, loss = 0.07956109\n",
      "Iteration 592, loss = 0.07922481\n",
      "Iteration 593, loss = 0.07889835\n",
      "Iteration 594, loss = 0.07859373\n",
      "Iteration 595, loss = 0.07821953\n",
      "Iteration 596, loss = 0.07790039\n",
      "Iteration 597, loss = 0.07757763\n",
      "Iteration 598, loss = 0.07731153\n",
      "Iteration 599, loss = 0.07700390\n",
      "Iteration 600, loss = 0.07665901\n",
      "Iteration 601, loss = 0.07638342\n",
      "Iteration 602, loss = 0.07609020\n",
      "Iteration 603, loss = 0.07581228\n",
      "Iteration 604, loss = 0.07551999\n",
      "Iteration 605, loss = 0.07523547\n",
      "Iteration 606, loss = 0.07494422\n",
      "Iteration 607, loss = 0.07465177\n",
      "Iteration 608, loss = 0.07440126\n",
      "Iteration 609, loss = 0.07410247\n",
      "Iteration 610, loss = 0.07381120\n",
      "Iteration 611, loss = 0.07355957\n",
      "Iteration 612, loss = 0.07327147\n",
      "Iteration 613, loss = 0.07300285\n",
      "Iteration 614, loss = 0.07271196\n",
      "Iteration 615, loss = 0.07248399\n",
      "Iteration 616, loss = 0.07218179\n",
      "Iteration 617, loss = 0.07191909\n",
      "Iteration 618, loss = 0.07167788\n",
      "Iteration 619, loss = 0.07138046\n",
      "Iteration 620, loss = 0.07109534\n",
      "Iteration 621, loss = 0.07083920\n",
      "Iteration 622, loss = 0.07062333\n",
      "Iteration 623, loss = 0.07032140\n",
      "Iteration 624, loss = 0.07007502\n",
      "Iteration 625, loss = 0.06981397\n",
      "Iteration 626, loss = 0.06957682\n",
      "Iteration 627, loss = 0.06930648\n",
      "Iteration 628, loss = 0.06905529\n",
      "Iteration 629, loss = 0.06879290\n",
      "Iteration 630, loss = 0.06858339\n",
      "Iteration 631, loss = 0.06832396\n",
      "Iteration 632, loss = 0.06811629\n",
      "Iteration 633, loss = 0.06786689\n",
      "Iteration 634, loss = 0.06765812\n",
      "Iteration 635, loss = 0.06739829\n",
      "Iteration 636, loss = 0.06716930\n",
      "Iteration 637, loss = 0.06693313\n",
      "Iteration 638, loss = 0.06672065\n",
      "Iteration 639, loss = 0.06651411\n",
      "Iteration 640, loss = 0.06631718\n",
      "Iteration 641, loss = 0.06604299\n",
      "Iteration 642, loss = 0.06584928\n",
      "Iteration 643, loss = 0.06562467\n",
      "Iteration 644, loss = 0.06543769\n",
      "Iteration 645, loss = 0.06523457\n",
      "Iteration 646, loss = 0.06500927\n",
      "Iteration 647, loss = 0.06480085\n",
      "Iteration 648, loss = 0.06459944\n",
      "Iteration 649, loss = 0.06440976\n",
      "Iteration 650, loss = 0.06420962\n",
      "Iteration 651, loss = 0.06399948\n",
      "Iteration 652, loss = 0.06379446\n",
      "Iteration 653, loss = 0.06364012\n",
      "Iteration 654, loss = 0.06337597\n",
      "Iteration 655, loss = 0.06319186\n",
      "Iteration 656, loss = 0.06299452\n",
      "Iteration 657, loss = 0.06281828\n",
      "Iteration 658, loss = 0.06260812\n",
      "Iteration 659, loss = 0.06238786\n",
      "Iteration 660, loss = 0.06220675\n",
      "Iteration 661, loss = 0.06199816\n",
      "Iteration 662, loss = 0.06179456\n",
      "Iteration 663, loss = 0.06162871\n",
      "Iteration 664, loss = 0.06141354\n",
      "Iteration 665, loss = 0.06123379\n",
      "Iteration 666, loss = 0.06099524\n",
      "Iteration 667, loss = 0.06078949\n",
      "Iteration 668, loss = 0.06061209\n",
      "Iteration 669, loss = 0.06040719\n",
      "Iteration 670, loss = 0.06021077\n",
      "Iteration 671, loss = 0.05999534\n",
      "Iteration 672, loss = 0.05981130\n",
      "Iteration 673, loss = 0.05961630\n",
      "Iteration 674, loss = 0.05941269\n",
      "Iteration 675, loss = 0.05919203\n",
      "Iteration 676, loss = 0.05903869\n",
      "Iteration 677, loss = 0.05879736\n",
      "Iteration 678, loss = 0.05857991\n",
      "Iteration 679, loss = 0.05836788\n",
      "Iteration 680, loss = 0.05817552\n",
      "Iteration 681, loss = 0.05797934\n",
      "Iteration 682, loss = 0.05776983\n",
      "Iteration 683, loss = 0.05756968\n",
      "Iteration 684, loss = 0.05740269\n",
      "Iteration 685, loss = 0.05718116\n",
      "Iteration 686, loss = 0.05699826\n",
      "Iteration 687, loss = 0.05679045\n",
      "Iteration 688, loss = 0.05659899\n",
      "Iteration 689, loss = 0.05640891\n",
      "Iteration 690, loss = 0.05620628\n",
      "Iteration 691, loss = 0.05603796\n",
      "Iteration 692, loss = 0.05582830\n",
      "Iteration 693, loss = 0.05562585\n",
      "Iteration 694, loss = 0.05545707\n",
      "Iteration 695, loss = 0.05522970\n",
      "Iteration 696, loss = 0.05507130\n",
      "Iteration 697, loss = 0.05488217\n",
      "Iteration 698, loss = 0.05463618\n",
      "Iteration 699, loss = 0.05444958\n",
      "Iteration 700, loss = 0.05428372\n",
      "Iteration 701, loss = 0.05408198\n",
      "Iteration 702, loss = 0.05390215\n",
      "Iteration 703, loss = 0.05369829\n",
      "Iteration 704, loss = 0.05352914\n",
      "Iteration 705, loss = 0.05334678\n",
      "Iteration 706, loss = 0.05317031\n",
      "Iteration 707, loss = 0.05297337\n",
      "Iteration 708, loss = 0.05278807\n",
      "Iteration 709, loss = 0.05262637\n",
      "Iteration 710, loss = 0.05244869\n",
      "Iteration 711, loss = 0.05227069\n",
      "Iteration 712, loss = 0.05208715\n",
      "Iteration 713, loss = 0.05192857\n",
      "Iteration 714, loss = 0.05174878\n",
      "Iteration 715, loss = 0.05154839\n",
      "Iteration 716, loss = 0.05137289\n",
      "Iteration 717, loss = 0.05122273\n",
      "Iteration 718, loss = 0.05103783\n",
      "Iteration 719, loss = 0.05087580\n",
      "Iteration 720, loss = 0.05067967\n",
      "Iteration 721, loss = 0.05049195\n",
      "Iteration 722, loss = 0.05036033\n",
      "Iteration 723, loss = 0.05015630\n",
      "Iteration 724, loss = 0.04996707\n",
      "Iteration 725, loss = 0.04981578\n",
      "Iteration 726, loss = 0.04966969\n",
      "Iteration 727, loss = 0.04945749\n",
      "Iteration 728, loss = 0.04932229\n",
      "Iteration 729, loss = 0.04912931\n",
      "Iteration 730, loss = 0.04897906\n",
      "Iteration 731, loss = 0.04880209\n",
      "Iteration 732, loss = 0.04862147\n",
      "Iteration 733, loss = 0.04849614\n",
      "Iteration 734, loss = 0.04834921\n",
      "Iteration 735, loss = 0.04815123\n",
      "Iteration 736, loss = 0.04800090\n",
      "Iteration 737, loss = 0.04786426\n",
      "Iteration 738, loss = 0.04769057\n",
      "Iteration 739, loss = 0.04752826\n",
      "Iteration 740, loss = 0.04740589\n",
      "Iteration 741, loss = 0.04721049\n",
      "Iteration 742, loss = 0.04707443\n",
      "Iteration 743, loss = 0.04691987\n",
      "Iteration 744, loss = 0.04676607\n",
      "Iteration 745, loss = 0.04662276\n",
      "Iteration 746, loss = 0.04647427\n",
      "Iteration 747, loss = 0.04632811\n",
      "Iteration 748, loss = 0.04617560\n",
      "Iteration 749, loss = 0.04604855\n",
      "Iteration 750, loss = 0.04590021\n",
      "Iteration 751, loss = 0.04575229\n",
      "Iteration 752, loss = 0.04564438\n",
      "Iteration 753, loss = 0.04544826\n",
      "Iteration 754, loss = 0.04532956\n",
      "Iteration 755, loss = 0.04516254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 756, loss = 0.04502482\n",
      "Iteration 757, loss = 0.04488085\n",
      "Iteration 758, loss = 0.04473429\n",
      "Iteration 759, loss = 0.04459336\n",
      "Iteration 760, loss = 0.04445780\n",
      "Iteration 761, loss = 0.04431714\n",
      "Iteration 762, loss = 0.04416361\n",
      "Iteration 763, loss = 0.04407571\n",
      "Iteration 764, loss = 0.04392747\n",
      "Iteration 765, loss = 0.04376977\n",
      "Iteration 766, loss = 0.04361732\n",
      "Iteration 767, loss = 0.04351500\n",
      "Iteration 768, loss = 0.04331802\n",
      "Iteration 769, loss = 0.04318755\n",
      "Iteration 770, loss = 0.04302557\n",
      "Iteration 771, loss = 0.04290371\n",
      "Iteration 772, loss = 0.04276888\n",
      "Iteration 773, loss = 0.04261523\n",
      "Iteration 774, loss = 0.04247952\n",
      "Iteration 775, loss = 0.04233309\n",
      "Iteration 776, loss = 0.04218528\n",
      "Iteration 777, loss = 0.04204360\n",
      "Iteration 778, loss = 0.04189921\n",
      "Iteration 779, loss = 0.04176562\n",
      "Iteration 780, loss = 0.04162542\n",
      "Iteration 781, loss = 0.04148508\n",
      "Iteration 782, loss = 0.04136800\n",
      "Iteration 783, loss = 0.04123437\n",
      "Iteration 784, loss = 0.04106983\n",
      "Iteration 785, loss = 0.04093428\n",
      "Iteration 786, loss = 0.04079170\n",
      "Iteration 787, loss = 0.04067122\n",
      "Iteration 788, loss = 0.04049839\n",
      "Iteration 789, loss = 0.04035979\n",
      "Iteration 790, loss = 0.04023094\n",
      "Iteration 791, loss = 0.04011024\n",
      "Iteration 792, loss = 0.03994709\n",
      "Iteration 793, loss = 0.03981181\n",
      "Iteration 794, loss = 0.03970165\n",
      "Iteration 795, loss = 0.03955961\n",
      "Iteration 796, loss = 0.03941341\n",
      "Iteration 797, loss = 0.03929783\n",
      "Iteration 798, loss = 0.03914978\n",
      "Iteration 799, loss = 0.03902223\n",
      "Iteration 800, loss = 0.03887823\n",
      "Iteration 801, loss = 0.03874898\n",
      "Iteration 802, loss = 0.03862072\n",
      "Iteration 803, loss = 0.03851229\n",
      "Iteration 804, loss = 0.03837258\n",
      "Iteration 805, loss = 0.03822570\n",
      "Iteration 806, loss = 0.03812776\n",
      "Iteration 807, loss = 0.03795278\n",
      "Iteration 808, loss = 0.03783097\n",
      "Iteration 809, loss = 0.03769297\n",
      "Iteration 810, loss = 0.03756900\n",
      "Iteration 811, loss = 0.03743364\n",
      "Iteration 812, loss = 0.03730497\n",
      "Iteration 813, loss = 0.03719673\n",
      "Iteration 814, loss = 0.03707752\n",
      "Iteration 815, loss = 0.03695321\n",
      "Iteration 816, loss = 0.03686254\n",
      "Iteration 817, loss = 0.03669177\n",
      "Iteration 818, loss = 0.03654655\n",
      "Iteration 819, loss = 0.03643240\n",
      "Iteration 820, loss = 0.03630883\n",
      "Iteration 821, loss = 0.03619508\n",
      "Iteration 822, loss = 0.03604448\n",
      "Iteration 823, loss = 0.03592561\n",
      "Iteration 824, loss = 0.03582598\n",
      "Iteration 825, loss = 0.03569092\n",
      "Iteration 826, loss = 0.03556768\n",
      "Iteration 827, loss = 0.03547249\n",
      "Iteration 828, loss = 0.03531404\n",
      "Iteration 829, loss = 0.03515713\n",
      "Iteration 830, loss = 0.03504095\n",
      "Iteration 831, loss = 0.03490412\n",
      "Iteration 832, loss = 0.03480062\n",
      "Iteration 833, loss = 0.03472449\n",
      "Iteration 834, loss = 0.03455146\n",
      "Iteration 835, loss = 0.03443282\n",
      "Iteration 836, loss = 0.03430390\n",
      "Iteration 837, loss = 0.03418845\n",
      "Iteration 838, loss = 0.03406952\n",
      "Iteration 839, loss = 0.03396476\n",
      "Iteration 840, loss = 0.03383670\n",
      "Iteration 841, loss = 0.03381048\n",
      "Iteration 842, loss = 0.03359755\n",
      "Iteration 843, loss = 0.03347919\n",
      "Iteration 844, loss = 0.03335730\n",
      "Iteration 845, loss = 0.03325309\n",
      "Iteration 846, loss = 0.03316141\n",
      "Iteration 847, loss = 0.03301148\n",
      "Iteration 848, loss = 0.03287180\n",
      "Iteration 849, loss = 0.03276212\n",
      "Iteration 850, loss = 0.03263015\n",
      "Iteration 851, loss = 0.03254779\n",
      "Iteration 852, loss = 0.03243866\n",
      "Iteration 853, loss = 0.03229240\n",
      "Iteration 854, loss = 0.03222495\n",
      "Iteration 855, loss = 0.03205671\n",
      "Iteration 856, loss = 0.03192906\n",
      "Iteration 857, loss = 0.03185757\n",
      "Iteration 858, loss = 0.03172049\n",
      "Iteration 859, loss = 0.03160142\n",
      "Iteration 860, loss = 0.03150322\n",
      "Iteration 861, loss = 0.03139254\n",
      "Iteration 862, loss = 0.03125629\n",
      "Iteration 863, loss = 0.03116005\n",
      "Iteration 864, loss = 0.03104859\n",
      "Iteration 865, loss = 0.03091877\n",
      "Iteration 866, loss = 0.03081544\n",
      "Iteration 867, loss = 0.03069675\n",
      "Iteration 868, loss = 0.03060627\n",
      "Iteration 869, loss = 0.03049006\n",
      "Iteration 870, loss = 0.03039633\n",
      "Iteration 871, loss = 0.03026045\n",
      "Iteration 872, loss = 0.03016255\n",
      "Iteration 873, loss = 0.03004729\n",
      "Iteration 874, loss = 0.02997002\n",
      "Iteration 875, loss = 0.02984417\n",
      "Iteration 876, loss = 0.02971951\n",
      "Iteration 877, loss = 0.02961897\n",
      "Iteration 878, loss = 0.02953828\n",
      "Iteration 879, loss = 0.02941119\n",
      "Iteration 880, loss = 0.02931350\n",
      "Iteration 881, loss = 0.02920604\n",
      "Iteration 882, loss = 0.02910729\n",
      "Iteration 883, loss = 0.02897902\n",
      "Iteration 884, loss = 0.02889007\n",
      "Iteration 885, loss = 0.02878221\n",
      "Iteration 886, loss = 0.02870153\n",
      "Iteration 887, loss = 0.02857166\n",
      "Iteration 888, loss = 0.02849202\n",
      "Iteration 889, loss = 0.02837871\n",
      "Iteration 890, loss = 0.02829193\n",
      "Iteration 891, loss = 0.02819362\n",
      "Iteration 892, loss = 0.02807539\n",
      "Iteration 893, loss = 0.02798225\n",
      "Iteration 894, loss = 0.02788346\n",
      "Iteration 895, loss = 0.02778051\n",
      "Iteration 896, loss = 0.02769032\n",
      "Iteration 897, loss = 0.02757254\n",
      "Iteration 898, loss = 0.02748529\n",
      "Iteration 899, loss = 0.02737342\n",
      "Iteration 900, loss = 0.02728584\n",
      "Iteration 901, loss = 0.02718882\n",
      "Iteration 902, loss = 0.02708573\n",
      "Iteration 903, loss = 0.02699796\n",
      "Iteration 904, loss = 0.02689718\n",
      "Iteration 905, loss = 0.02678835\n",
      "Iteration 906, loss = 0.02672213\n",
      "Iteration 907, loss = 0.02661127\n",
      "Iteration 908, loss = 0.02650615\n",
      "Iteration 909, loss = 0.02642674\n",
      "Iteration 910, loss = 0.02632045\n",
      "Iteration 911, loss = 0.02624430\n",
      "Iteration 912, loss = 0.02614665\n",
      "Iteration 913, loss = 0.02603962\n",
      "Iteration 914, loss = 0.02596623\n",
      "Iteration 915, loss = 0.02587545\n",
      "Iteration 916, loss = 0.02577381\n",
      "Iteration 917, loss = 0.02566137\n",
      "Iteration 918, loss = 0.02557984\n",
      "Iteration 919, loss = 0.02548762\n",
      "Iteration 920, loss = 0.02542049\n",
      "Iteration 921, loss = 0.02533002\n",
      "Iteration 922, loss = 0.02522201\n",
      "Iteration 923, loss = 0.02513526\n",
      "Iteration 924, loss = 0.02505122\n",
      "Iteration 925, loss = 0.02496966\n",
      "Iteration 926, loss = 0.02487584\n",
      "Iteration 927, loss = 0.02479309\n",
      "Iteration 928, loss = 0.02471051\n",
      "Iteration 929, loss = 0.02460821\n",
      "Iteration 930, loss = 0.02453278\n",
      "Iteration 931, loss = 0.02445123\n",
      "Iteration 932, loss = 0.02435820\n",
      "Iteration 933, loss = 0.02425404\n",
      "Iteration 934, loss = 0.02418821\n",
      "Iteration 935, loss = 0.02412509\n",
      "Iteration 936, loss = 0.02400902\n",
      "Iteration 937, loss = 0.02392147\n",
      "Iteration 938, loss = 0.02384740\n",
      "Iteration 939, loss = 0.02376276\n",
      "Iteration 940, loss = 0.02368671\n",
      "Iteration 941, loss = 0.02358045\n",
      "Iteration 942, loss = 0.02350355\n",
      "Iteration 943, loss = 0.02342135\n",
      "Iteration 944, loss = 0.02336349\n",
      "Iteration 945, loss = 0.02325603\n",
      "Iteration 946, loss = 0.02317452\n",
      "Iteration 947, loss = 0.02307523\n",
      "Iteration 948, loss = 0.02300533\n",
      "Iteration 949, loss = 0.02300873\n",
      "Iteration 950, loss = 0.02284433\n",
      "Iteration 951, loss = 0.02274661\n",
      "Iteration 952, loss = 0.02272919\n",
      "Iteration 953, loss = 0.02259882\n",
      "Iteration 954, loss = 0.02251866\n",
      "Iteration 955, loss = 0.02243122\n",
      "Iteration 956, loss = 0.02239476\n",
      "Iteration 957, loss = 0.02228833\n",
      "Iteration 958, loss = 0.02219958\n",
      "Iteration 959, loss = 0.02212340\n",
      "Iteration 960, loss = 0.02204503\n",
      "Iteration 961, loss = 0.02197445\n",
      "Iteration 962, loss = 0.02190107\n",
      "Iteration 963, loss = 0.02182029\n",
      "Iteration 964, loss = 0.02174367\n",
      "Iteration 965, loss = 0.02166104\n",
      "Iteration 966, loss = 0.02159117\n",
      "Iteration 967, loss = 0.02156049\n",
      "Iteration 968, loss = 0.02148397\n",
      "Iteration 969, loss = 0.02137497\n",
      "Iteration 970, loss = 0.02132259\n",
      "Iteration 971, loss = 0.02121586\n",
      "Iteration 972, loss = 0.02115901\n",
      "Iteration 973, loss = 0.02111804\n",
      "Iteration 974, loss = 0.02099106\n",
      "Iteration 975, loss = 0.02093247\n",
      "Iteration 976, loss = 0.02087149\n",
      "Iteration 977, loss = 0.02078487\n",
      "Iteration 978, loss = 0.02073977\n",
      "Iteration 979, loss = 0.02067946\n",
      "Iteration 980, loss = 0.02058420\n",
      "Iteration 981, loss = 0.02052005\n",
      "Iteration 982, loss = 0.02041503\n",
      "Iteration 983, loss = 0.02033059\n",
      "Iteration 984, loss = 0.02027277\n",
      "Iteration 985, loss = 0.02020658\n",
      "Iteration 986, loss = 0.02013158\n",
      "Iteration 987, loss = 0.02006471\n",
      "Iteration 988, loss = 0.01999263\n",
      "Iteration 989, loss = 0.01989948\n",
      "Iteration 990, loss = 0.01984715\n",
      "Iteration 991, loss = 0.01981291\n",
      "Iteration 992, loss = 0.01972615\n",
      "Iteration 993, loss = 0.01962787\n",
      "Iteration 994, loss = 0.01955752\n",
      "Iteration 995, loss = 0.01950371\n",
      "Iteration 996, loss = 0.01943390\n",
      "Iteration 997, loss = 0.01940170\n",
      "Iteration 998, loss = 0.01929855\n",
      "Iteration 999, loss = 0.01921955\n",
      "Iteration 1000, loss = 0.01916921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1000, tol=1e-05, verbose=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network = MLPClassifier(max_iter=1000, verbose=True,tol=0.0000100, hidden_layer_sizes=(2,2))#aumentando a tolerancia pra ver se o erro diminui\n",
    "neural_network.fit(x_df_train, y_df_train)                    # hidden layer (100,100) significa que temos 2 camadas com 100 neuronios cada\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes = neural_network.predict(x_df_test)\n",
    "previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.996"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_df_test, previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       436\n",
      "           1       0.98      0.98      0.98        64\n",
      "\n",
      "    accuracy                           1.00       500\n",
      "   macro avg       0.99      0.99      0.99       500\n",
      "weighted avg       1.00      1.00      1.00       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_df_test, previsoes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('census.pkl', 'rb') as f:\n",
    "    x_df_train, y_df_train, x_df_test, y_df_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27676, 108)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hidden layer size\n",
    "(108+1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.41758301\n",
      "Iteration 2, loss = 0.33025875\n",
      "Iteration 3, loss = 0.31730405\n",
      "Iteration 4, loss = 0.30964575\n",
      "Iteration 5, loss = 0.30369766\n",
      "Iteration 6, loss = 0.29927631\n",
      "Iteration 7, loss = 0.29611590\n",
      "Iteration 8, loss = 0.29318700\n",
      "Iteration 9, loss = 0.29045462\n",
      "Iteration 10, loss = 0.28802768\n",
      "Iteration 11, loss = 0.28658234\n",
      "Iteration 12, loss = 0.28374992\n",
      "Iteration 13, loss = 0.28233889\n",
      "Iteration 14, loss = 0.28039215\n",
      "Iteration 15, loss = 0.27890077\n",
      "Iteration 16, loss = 0.27702232\n",
      "Iteration 17, loss = 0.27592822\n",
      "Iteration 18, loss = 0.27518167\n",
      "Iteration 19, loss = 0.27221542\n",
      "Iteration 20, loss = 0.27156222\n",
      "Iteration 21, loss = 0.26968185\n",
      "Iteration 22, loss = 0.26777587\n",
      "Iteration 23, loss = 0.26613316\n",
      "Iteration 24, loss = 0.26529471\n",
      "Iteration 25, loss = 0.26376765\n",
      "Iteration 26, loss = 0.26290485\n",
      "Iteration 27, loss = 0.26063976\n",
      "Iteration 28, loss = 0.25952721\n",
      "Iteration 29, loss = 0.25726743\n",
      "Iteration 30, loss = 0.25641742\n",
      "Iteration 31, loss = 0.25489662\n",
      "Iteration 32, loss = 0.25375726\n",
      "Iteration 33, loss = 0.25313083\n",
      "Iteration 34, loss = 0.25114440\n",
      "Iteration 35, loss = 0.24979981\n",
      "Iteration 36, loss = 0.24855444\n",
      "Iteration 37, loss = 0.24661684\n",
      "Iteration 38, loss = 0.24562397\n",
      "Iteration 39, loss = 0.24471123\n",
      "Iteration 40, loss = 0.24344696\n",
      "Iteration 41, loss = 0.24227132\n",
      "Iteration 42, loss = 0.24146747\n",
      "Iteration 43, loss = 0.23976252\n",
      "Iteration 44, loss = 0.23925253\n",
      "Iteration 45, loss = 0.23853978\n",
      "Iteration 46, loss = 0.23637102\n",
      "Iteration 47, loss = 0.23626137\n",
      "Iteration 48, loss = 0.23552361\n",
      "Iteration 49, loss = 0.23400041\n",
      "Iteration 50, loss = 0.23268590\n",
      "Iteration 51, loss = 0.23231988\n",
      "Iteration 52, loss = 0.23220036\n",
      "Iteration 53, loss = 0.23005665\n",
      "Iteration 54, loss = 0.22914334\n",
      "Iteration 55, loss = 0.22861660\n",
      "Iteration 56, loss = 0.22822787\n",
      "Iteration 57, loss = 0.22730292\n",
      "Iteration 58, loss = 0.22609041\n",
      "Iteration 59, loss = 0.22549575\n",
      "Iteration 60, loss = 0.22387767\n",
      "Iteration 61, loss = 0.22396077\n",
      "Iteration 62, loss = 0.22345653\n",
      "Iteration 63, loss = 0.22223235\n",
      "Iteration 64, loss = 0.22072191\n",
      "Iteration 65, loss = 0.21974157\n",
      "Iteration 66, loss = 0.21987012\n",
      "Iteration 67, loss = 0.21830734\n",
      "Iteration 68, loss = 0.21810374\n",
      "Iteration 69, loss = 0.21650485\n",
      "Iteration 70, loss = 0.21618712\n",
      "Iteration 71, loss = 0.21573302\n",
      "Iteration 72, loss = 0.21452880\n",
      "Iteration 73, loss = 0.21425714\n",
      "Iteration 74, loss = 0.21403750\n",
      "Iteration 75, loss = 0.21259162\n",
      "Iteration 76, loss = 0.21162908\n",
      "Iteration 77, loss = 0.21068193\n",
      "Iteration 78, loss = 0.21018284\n",
      "Iteration 79, loss = 0.21142161\n",
      "Iteration 80, loss = 0.20992752\n",
      "Iteration 81, loss = 0.20898107\n",
      "Iteration 82, loss = 0.20853278\n",
      "Iteration 83, loss = 0.20832302\n",
      "Iteration 84, loss = 0.20657492\n",
      "Iteration 85, loss = 0.20630575\n",
      "Iteration 86, loss = 0.20546214\n",
      "Iteration 87, loss = 0.20578907\n",
      "Iteration 88, loss = 0.20483809\n",
      "Iteration 89, loss = 0.20503126\n",
      "Iteration 90, loss = 0.20398972\n",
      "Iteration 91, loss = 0.20325046\n",
      "Iteration 92, loss = 0.20294309\n",
      "Iteration 93, loss = 0.20170927\n",
      "Iteration 94, loss = 0.20204849\n",
      "Iteration 95, loss = 0.20095798\n",
      "Iteration 96, loss = 0.19976417\n",
      "Iteration 97, loss = 0.20038532\n",
      "Iteration 98, loss = 0.20012247\n",
      "Iteration 99, loss = 0.19832633\n",
      "Iteration 100, loss = 0.19776460\n",
      "Iteration 101, loss = 0.19777991\n",
      "Iteration 102, loss = 0.19734966\n",
      "Iteration 103, loss = 0.19712821\n",
      "Iteration 104, loss = 0.19655515\n",
      "Iteration 105, loss = 0.19563278\n",
      "Iteration 106, loss = 0.19606875\n",
      "Iteration 107, loss = 0.19576538\n",
      "Iteration 108, loss = 0.19420771\n",
      "Iteration 109, loss = 0.19520995\n",
      "Iteration 110, loss = 0.19345141\n",
      "Iteration 111, loss = 0.19222255\n",
      "Iteration 112, loss = 0.19463415\n",
      "Iteration 113, loss = 0.19171640\n",
      "Iteration 114, loss = 0.19196563\n",
      "Iteration 115, loss = 0.19130562\n",
      "Iteration 116, loss = 0.19138726\n",
      "Iteration 117, loss = 0.19025586\n",
      "Iteration 118, loss = 0.19069684\n",
      "Iteration 119, loss = 0.18938412\n",
      "Iteration 120, loss = 0.18944766\n",
      "Iteration 121, loss = 0.18971190\n",
      "Iteration 122, loss = 0.18772774\n",
      "Iteration 123, loss = 0.18797395\n",
      "Iteration 124, loss = 0.18792176\n",
      "Iteration 125, loss = 0.18880616\n",
      "Iteration 126, loss = 0.18685236\n",
      "Iteration 127, loss = 0.18580027\n",
      "Iteration 128, loss = 0.18553979\n",
      "Iteration 129, loss = 0.18552093\n",
      "Iteration 130, loss = 0.18496005\n",
      "Iteration 131, loss = 0.18447676\n",
      "Iteration 132, loss = 0.18339131\n",
      "Iteration 133, loss = 0.18414127\n",
      "Iteration 134, loss = 0.18310095\n",
      "Iteration 135, loss = 0.18483446\n",
      "Iteration 136, loss = 0.18269486\n",
      "Iteration 137, loss = 0.18176481\n",
      "Iteration 138, loss = 0.18260591\n",
      "Iteration 139, loss = 0.18243908\n",
      "Iteration 140, loss = 0.18089638\n",
      "Iteration 141, loss = 0.18066583\n",
      "Iteration 142, loss = 0.18015962\n",
      "Iteration 143, loss = 0.17994801\n",
      "Iteration 144, loss = 0.17894125\n",
      "Iteration 145, loss = 0.17835874\n",
      "Iteration 146, loss = 0.17764206\n",
      "Iteration 147, loss = 0.17771420\n",
      "Iteration 148, loss = 0.17883896\n",
      "Iteration 149, loss = 0.17881326\n",
      "Iteration 150, loss = 0.17786694\n",
      "Iteration 151, loss = 0.17686669\n",
      "Iteration 152, loss = 0.17657050\n",
      "Iteration 153, loss = 0.17582893\n",
      "Iteration 154, loss = 0.17810864\n",
      "Iteration 155, loss = 0.17729902\n",
      "Iteration 156, loss = 0.17612466\n",
      "Iteration 157, loss = 0.17604648\n",
      "Iteration 158, loss = 0.17558921\n",
      "Iteration 159, loss = 0.17541351\n",
      "Iteration 160, loss = 0.17338169\n",
      "Iteration 161, loss = 0.17477456\n",
      "Iteration 162, loss = 0.17282523\n",
      "Iteration 163, loss = 0.17233376\n",
      "Iteration 164, loss = 0.17318338\n",
      "Iteration 165, loss = 0.17260008\n",
      "Iteration 166, loss = 0.17451273\n",
      "Iteration 167, loss = 0.17238510\n",
      "Iteration 168, loss = 0.17190611\n",
      "Iteration 169, loss = 0.17416544\n",
      "Iteration 170, loss = 0.17131255\n",
      "Iteration 171, loss = 0.17072875\n",
      "Iteration 172, loss = 0.17060973\n",
      "Iteration 173, loss = 0.17018420\n",
      "Iteration 174, loss = 0.16986682\n",
      "Iteration 175, loss = 0.16996057\n",
      "Iteration 176, loss = 0.16937525\n",
      "Iteration 177, loss = 0.16968457\n",
      "Iteration 178, loss = 0.16850187\n",
      "Iteration 179, loss = 0.17173837\n",
      "Iteration 180, loss = 0.16741208\n",
      "Iteration 181, loss = 0.17075913\n",
      "Iteration 182, loss = 0.16833873\n",
      "Iteration 183, loss = 0.16700863\n",
      "Iteration 184, loss = 0.16705614\n",
      "Iteration 185, loss = 0.16730841\n",
      "Iteration 186, loss = 0.16729396\n",
      "Iteration 187, loss = 0.16596205\n",
      "Iteration 188, loss = 0.16714293\n",
      "Iteration 189, loss = 0.16758467\n",
      "Iteration 190, loss = 0.16767795\n",
      "Iteration 191, loss = 0.16710221\n",
      "Iteration 192, loss = 0.16543860\n",
      "Iteration 193, loss = 0.16638906\n",
      "Iteration 194, loss = 0.16657324\n",
      "Iteration 195, loss = 0.16845005\n",
      "Iteration 196, loss = 0.16484917\n",
      "Iteration 197, loss = 0.16365959\n",
      "Iteration 198, loss = 0.16422766\n",
      "Iteration 199, loss = 0.16335881\n",
      "Iteration 200, loss = 0.16489978\n",
      "Iteration 201, loss = 0.16373422\n",
      "Iteration 202, loss = 0.16321252\n",
      "Iteration 203, loss = 0.16278030\n",
      "Iteration 204, loss = 0.16140955\n",
      "Iteration 205, loss = 0.16233783\n",
      "Iteration 206, loss = 0.16438250\n",
      "Iteration 207, loss = 0.16228959\n",
      "Iteration 208, loss = 0.16262503\n",
      "Iteration 209, loss = 0.16246041\n",
      "Iteration 210, loss = 0.16180784\n",
      "Iteration 211, loss = 0.16157832\n",
      "Iteration 212, loss = 0.16228260\n",
      "Iteration 213, loss = 0.15995051\n",
      "Iteration 214, loss = 0.15935190\n",
      "Iteration 215, loss = 0.16091726\n",
      "Iteration 216, loss = 0.16092940\n",
      "Iteration 217, loss = 0.16072872\n",
      "Iteration 218, loss = 0.15916926\n",
      "Iteration 219, loss = 0.16103559\n",
      "Iteration 220, loss = 0.15917231\n",
      "Iteration 221, loss = 0.15902030\n",
      "Iteration 222, loss = 0.15967152\n",
      "Iteration 223, loss = 0.15839431\n",
      "Iteration 224, loss = 0.15725666\n",
      "Iteration 225, loss = 0.15845710\n",
      "Iteration 226, loss = 0.15890771\n",
      "Iteration 227, loss = 0.15940388\n",
      "Iteration 228, loss = 0.15955622\n",
      "Iteration 229, loss = 0.15990936\n",
      "Iteration 230, loss = 0.15829474\n",
      "Iteration 231, loss = 0.15729918\n",
      "Iteration 232, loss = 0.15704178\n",
      "Iteration 233, loss = 0.15795060\n",
      "Iteration 234, loss = 0.15715546\n",
      "Iteration 235, loss = 0.15628764\n",
      "Iteration 236, loss = 0.15688014\n",
      "Iteration 237, loss = 0.15563277\n",
      "Iteration 238, loss = 0.15568814\n",
      "Iteration 239, loss = 0.15546099\n",
      "Iteration 240, loss = 0.15664064\n",
      "Iteration 241, loss = 0.15507154\n",
      "Iteration 242, loss = 0.15573391\n",
      "Iteration 243, loss = 0.15803612\n",
      "Iteration 244, loss = 0.15516066\n",
      "Iteration 245, loss = 0.15388841\n",
      "Iteration 246, loss = 0.15425222\n",
      "Iteration 247, loss = 0.15568307\n",
      "Iteration 248, loss = 0.15571703\n",
      "Iteration 249, loss = 0.15432960\n",
      "Iteration 250, loss = 0.15266010\n",
      "Iteration 251, loss = 0.15412457\n",
      "Iteration 252, loss = 0.15347562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.15271160\n",
      "Iteration 254, loss = 0.15484693\n",
      "Iteration 255, loss = 0.15266564\n",
      "Iteration 256, loss = 0.15440565\n",
      "Iteration 257, loss = 0.15291413\n",
      "Iteration 258, loss = 0.15301758\n",
      "Iteration 259, loss = 0.15207113\n",
      "Iteration 260, loss = 0.15289914\n",
      "Iteration 261, loss = 0.15514596\n",
      "Iteration 262, loss = 0.15163168\n",
      "Iteration 263, loss = 0.15148264\n",
      "Iteration 264, loss = 0.15292856\n",
      "Iteration 265, loss = 0.15218300\n",
      "Iteration 266, loss = 0.15153581\n",
      "Iteration 267, loss = 0.15096189\n",
      "Iteration 268, loss = 0.15226867\n",
      "Iteration 269, loss = 0.15207123\n",
      "Iteration 270, loss = 0.15137895\n",
      "Iteration 271, loss = 0.15060792\n",
      "Iteration 272, loss = 0.15077759\n",
      "Iteration 273, loss = 0.15010057\n",
      "Iteration 274, loss = 0.15123283\n",
      "Iteration 275, loss = 0.14967968\n",
      "Iteration 276, loss = 0.15014879\n",
      "Iteration 277, loss = 0.14886275\n",
      "Iteration 278, loss = 0.15041472\n",
      "Iteration 279, loss = 0.14902032\n",
      "Iteration 280, loss = 0.14885586\n",
      "Iteration 281, loss = 0.14934019\n",
      "Iteration 282, loss = 0.14842725\n",
      "Iteration 283, loss = 0.14794806\n",
      "Iteration 284, loss = 0.14821984\n",
      "Iteration 285, loss = 0.14969775\n",
      "Iteration 286, loss = 0.14829869\n",
      "Iteration 287, loss = 0.14915274\n",
      "Iteration 288, loss = 0.14857631\n",
      "Iteration 289, loss = 0.14754132\n",
      "Iteration 290, loss = 0.14800590\n",
      "Iteration 291, loss = 0.14891474\n",
      "Iteration 292, loss = 0.14684530\n",
      "Iteration 293, loss = 0.14830484\n",
      "Iteration 294, loss = 0.14790150\n",
      "Iteration 295, loss = 0.14789248\n",
      "Iteration 296, loss = 0.14934612\n",
      "Iteration 297, loss = 0.14783578\n",
      "Iteration 298, loss = 0.14992083\n",
      "Iteration 299, loss = 0.14662850\n",
      "Iteration 300, loss = 0.14551262\n",
      "Iteration 301, loss = 0.14620527\n",
      "Iteration 302, loss = 0.14436779\n",
      "Iteration 303, loss = 0.14812522\n",
      "Iteration 304, loss = 0.14678040\n",
      "Iteration 305, loss = 0.14478535\n",
      "Iteration 306, loss = 0.14502745\n",
      "Iteration 307, loss = 0.14498961\n",
      "Iteration 308, loss = 0.14505093\n",
      "Iteration 309, loss = 0.14488917\n",
      "Iteration 310, loss = 0.14611566\n",
      "Iteration 311, loss = 0.14687642\n",
      "Iteration 312, loss = 0.14376407\n",
      "Iteration 313, loss = 0.14546658\n",
      "Iteration 314, loss = 0.14452146\n",
      "Iteration 315, loss = 0.14482163\n",
      "Iteration 316, loss = 0.14593013\n",
      "Iteration 317, loss = 0.14482775\n",
      "Iteration 318, loss = 0.14392811\n",
      "Iteration 319, loss = 0.14384235\n",
      "Iteration 320, loss = 0.14497406\n",
      "Iteration 321, loss = 0.14418715\n",
      "Iteration 322, loss = 0.14359598\n",
      "Iteration 323, loss = 0.14243379\n",
      "Iteration 324, loss = 0.14252019\n",
      "Iteration 325, loss = 0.14348712\n",
      "Iteration 326, loss = 0.14203197\n",
      "Iteration 327, loss = 0.14293117\n",
      "Iteration 328, loss = 0.14505122\n",
      "Iteration 329, loss = 0.14256690\n",
      "Iteration 330, loss = 0.14180150\n",
      "Iteration 331, loss = 0.14121313\n",
      "Iteration 332, loss = 0.14214176\n",
      "Iteration 333, loss = 0.14375679\n",
      "Iteration 334, loss = 0.14226449\n",
      "Iteration 335, loss = 0.14170727\n",
      "Iteration 336, loss = 0.14119202\n",
      "Iteration 337, loss = 0.14240573\n",
      "Iteration 338, loss = 0.14337231\n",
      "Iteration 339, loss = 0.14032206\n",
      "Iteration 340, loss = 0.14228421\n",
      "Iteration 341, loss = 0.14244538\n",
      "Iteration 342, loss = 0.14372324\n",
      "Iteration 343, loss = 0.14205158\n",
      "Iteration 344, loss = 0.14267577\n",
      "Iteration 345, loss = 0.14104188\n",
      "Iteration 346, loss = 0.14328058\n",
      "Iteration 347, loss = 0.14077711\n",
      "Iteration 348, loss = 0.14087272\n",
      "Iteration 349, loss = 0.14005909\n",
      "Iteration 350, loss = 0.14204481\n",
      "Iteration 351, loss = 0.13987908\n",
      "Iteration 352, loss = 0.13917960\n",
      "Iteration 353, loss = 0.14226010\n",
      "Iteration 354, loss = 0.13932134\n",
      "Iteration 355, loss = 0.13922056\n",
      "Iteration 356, loss = 0.14044001\n",
      "Iteration 357, loss = 0.13892812\n",
      "Iteration 358, loss = 0.13895490\n",
      "Iteration 359, loss = 0.14127182\n",
      "Iteration 360, loss = 0.13975603\n",
      "Iteration 361, loss = 0.13859523\n",
      "Iteration 362, loss = 0.14044192\n",
      "Iteration 363, loss = 0.13990076\n",
      "Iteration 364, loss = 0.14207876\n",
      "Iteration 365, loss = 0.13917045\n",
      "Iteration 366, loss = 0.13809998\n",
      "Iteration 367, loss = 0.13915986\n",
      "Iteration 368, loss = 0.13825615\n",
      "Iteration 369, loss = 0.13625327\n",
      "Iteration 370, loss = 0.13856816\n",
      "Iteration 371, loss = 0.14066103\n",
      "Iteration 372, loss = 0.13982750\n",
      "Iteration 373, loss = 0.13768157\n",
      "Iteration 374, loss = 0.13504287\n",
      "Iteration 375, loss = 0.13788730\n",
      "Iteration 376, loss = 0.13710035\n",
      "Iteration 377, loss = 0.13707843\n",
      "Iteration 378, loss = 0.13751885\n",
      "Iteration 379, loss = 0.13678652\n",
      "Iteration 380, loss = 0.13730460\n",
      "Iteration 381, loss = 0.13717583\n",
      "Iteration 382, loss = 0.13770266\n",
      "Iteration 383, loss = 0.13629974\n",
      "Iteration 384, loss = 0.13648844\n",
      "Iteration 385, loss = 0.13742815\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1000, tol=1e-05,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network = MLPClassifier(verbose=True, max_iter=1000, tol=0.000010,\n",
    "                              hidden_layer_sizes = (55,55))\n",
    "neural_network.fit(x_df_train, y_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' <=50K', ' <=50K', ' <=50K', ..., ' <=50K', ' <=50K', ' >50K'],\n",
       "      dtype='<U6')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes = neural_network.predict(x_df_test)\n",
    "previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8182190378710338"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_df_test, previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.87      0.89      0.88      3693\n",
      "        >50K       0.64      0.60      0.62      1192\n",
      "\n",
      "    accuracy                           0.82      4885\n",
      "   macro avg       0.75      0.74      0.75      4885\n",
      "weighted avg       0.81      0.82      0.82      4885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_df_test, previsoes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
